{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba41be9",
   "metadata": {},
   "source": [
    "### Target data distribution p1(x1), original code used a checkerboard sampling method, replace with actual target data \n",
    "\n",
    "Done! I've converted the notebook to conditional flow matching. Here's what changed:\n",
    "\n",
    "Data preparation (cell 2): Now keeps only tail data and normalizes HiggsM values for conditioning.\n",
    "\n",
    "Model (cell 7): ConditionalMLP now accepts three inputs:\n",
    "\n",
    "x: frequency data\n",
    "t: time step\n",
    "cond: normalized HiggsM value\n",
    "The HiggsM conditioning information is projected and added to the network, allowing the model to learn how the velocity field varies with HiggsM.\n",
    "\n",
    "Training (cell 9): Now passes the corresponding HiggsM value for each frequency sample during training.\n",
    "\n",
    "Sampling (cell 12): Samples across the entire HiggsM range including the pole region. The model uses learned patterns from tail data to extrapolate predictions into the pole region where it has no direct training data.\n",
    "\n",
    "The key insight: the model learns \"how frequencies behave at this HiggsM value\" from the tails, then applies that learned pattern to generate predictions in the pole region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "raw_df = pd.read_hdf('data_files/new_Input_NonResonant_yy_25th_January2026.h5', key='VBF_Polarisation_Tree')\n",
    "\n",
    "# Define bounds and number of bins\n",
    "lower_bound = 115000\n",
    "upper_bound = 135000\n",
    "num_bins = 1100\n",
    "\n",
    "# Creating frequency distribution for HiggsM\n",
    "freq = pd.cut(raw_df['HiggsM'], bins=num_bins)\n",
    "freq_counts = freq.value_counts().sort_index()\n",
    "higgsM_values = [interval.mid for interval in freq_counts.index]\n",
    "frequencies = freq_counts.values.tolist()\n",
    "\n",
    "# Create DataFrame for ML Model\n",
    "model_df = pd.DataFrame({'HiggsM': higgsM_values, 'Frequency': frequencies})\n",
    "mask1 = model_df['HiggsM'] > upper_bound\n",
    "mask2 = model_df['HiggsM'] < lower_bound\n",
    "model_df_tails = model_df[mask1 | mask2]\n",
    "\n",
    "# Prepare data for conditional flow matching\n",
    "# Keep tail data with HiggsM values as conditioning information\n",
    "sampled_points = model_df_tails[['HiggsM', 'Frequency']].to_numpy()\n",
    "\n",
    "# Normalize both HiggsM and Frequency to [0, 1]\n",
    "HiggsM_min = model_df['HiggsM'].min()\n",
    "HiggsM_max = model_df['HiggsM'].max()\n",
    "HiggsM_normalized = (sampled_points[:, 0] - HiggsM_min) / (HiggsM_max - HiggsM_min)\n",
    "\n",
    "Freq_min = sampled_points[:, 1].min()\n",
    "Freq_max = sampled_points[:, 1].max()\n",
    "Freq_normalized = (sampled_points[:, 1] - Freq_min) / (Freq_max - Freq_min)\n",
    "\n",
    "print(f\"Tail data shape: {sampled_points.shape}\")\n",
    "print(f\"HiggsM range: {HiggsM_min:.0f} to {HiggsM_max:.0f}\")\n",
    "print(f\"Frequency range: {Freq_min:.0f} to {Freq_max:.0f}\")\n",
    "print(f\"Number of tail points: {len(sampled_points)}\")\n",
    "\n",
    "# Visualize tail data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(sampled_points[:, 0], sampled_points[:, 1], alpha=0.6)\n",
    "plt.xlabel('HiggsM')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(lower_bound, color='red', linestyle='--', label='Pole region bounds')\n",
    "plt.axvline(upper_bound, color='red', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Tail Data Only (will train conditional model)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad6471",
   "metadata": {},
   "source": [
    "### Random noise data points to start with, this is the initial distribution p0(x0), original code used random noise, replace with the Higgs tail data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6e028",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### This part shouldn't change for now, additional features will be added later on for more advanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7990f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, channels=512):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(channels, channels)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.ff(x))\n",
    "\n",
    "class ConditionalMLP(nn.Module):\n",
    "    def __init__(self, channels_data=1, layers=5, channels=512, channels_t=512, channels_cond=128):\n",
    "        super().__init__()\n",
    "        self.channels_t = channels_t\n",
    "        self.channels_cond = channels_cond\n",
    "        self.in_projection = nn.Linear(channels_data, channels)\n",
    "        self.t_projection = nn.Linear(channels_t, channels)\n",
    "\n",
    "        # Condition (HiggsM) projection\n",
    "        self.cond_projection = nn.Linear(1, channels_cond)\n",
    "        self.cond_to_channels = nn.Linear(channels_cond, channels)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(channels) for _ in range(layers)\n",
    "        ])\n",
    "        self.out_projection = nn.Linear(channels, channels_data)\n",
    "\n",
    "    def gen_t_embedding(self, t, max_positions=10000):\n",
    "        t = t * max_positions\n",
    "        half_dim = self.channels_t // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=t.device).float().mul(-emb).exp()\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.channels_t % 2 == 1:  # zero pad\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x, t, cond):\n",
    "        # x: [batch, 1] (frequency only)\n",
    "        # t: [batch] (time)\n",
    "        # cond: [batch, 1] (HiggsM normalized)\n",
    "        x = self.in_projection(x)\n",
    "        t = self.gen_t_embedding(t)\n",
    "        t = self.t_projection(t)\n",
    "        cond = self.cond_projection(cond)  # [batch, channels_cond]\n",
    "        cond = self.cond_to_channels(cond)  # [batch, channels]\n",
    "        # Combine: data + time + condition\n",
    "        x = x + t + cond\n",
    "        x = self.blocks(x)\n",
    "        x = self.out_projection(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalMLP(channels_data=1, layers=5, channels=512, channels_cond=128)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c77e33",
   "metadata": {},
   "source": [
    "channels_data is the dimensionality of the data the model is trying to predict.\n",
    "\n",
    "In the original checkerboard example, channels_data=2 because each data point had 2 features: (x, y) coordinates.\n",
    "\n",
    "In your Higgs background model, channels_data=1 because:\n",
    "\n",
    "Input data: Only frequency (1-dimensional)\n",
    "Output: Predicted frequency (1-dimensional)\n",
    "HiggsM: Now a separate conditioning input, not part of the main data\n",
    "So the flow matching pipeline is:\n",
    "\n",
    "x: 1D frequency values\n",
    "t: time parameter\n",
    "cond: HiggsM value (controls what the model predicts)\n",
    "The model learns to transform 1D noise â†’ 1D frequency predictions, conditioned on where you are in the HiggsM spectrum.\n",
    "\n",
    "If you later wanted to add more features (e.g., predict frequency and some other variable simultaneously), you'd increase channels_data accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30aba0",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Again this shouldn't change much for now aside from the distributions being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate HiggsM and Frequency data (both normalized)\n",
    "freq_data = torch.Tensor(Freq_normalized[:, None])  # [N, 1] - normalized to [0, 1]\n",
    "higgsM_data = torch.Tensor(HiggsM_normalized[:, None])  # [N, 1] - normalized to [0, 1]\n",
    "\n",
    "training_steps = 3000\n",
    "batch_size = 64\n",
    "pbar = tqdm.tqdm(range(training_steps))\n",
    "losses = []\n",
    "for i in pbar:\n",
    "    # Sample random indices\n",
    "    idx = torch.randint(freq_data.size(0), (batch_size,))\n",
    "    # Get frequency data (x1 is the target tail frequency)\n",
    "    x1 = freq_data[idx]  # [batch, 1]\n",
    "    # Get corresponding HiggsM conditioning values\n",
    "    cond = higgsM_data[idx]  # [batch, 1]\n",
    "\n",
    "    x0 = torch.randn_like(x1)\n",
    "    target = x1 - x0\n",
    "    t = torch.rand(x1.size(0))\n",
    "    xt = (1 - t[:, None]) * x0 + t[:, None] * x1\n",
    "\n",
    "    # Model prediction (now conditioned on HiggsM)\n",
    "    pred = model(xt, t, cond)\n",
    "\n",
    "    loss = ((target - pred)**2).mean()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_postfix(loss=loss.item())\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708bf68",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "### Same thing as Model and Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b22ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# Sample across the full HiggsM range (tails + pole region)\n",
    "num_samples_per_region = 200\n",
    "steps = 1000\n",
    "\n",
    "# Create HiggsM values for sampling: tails + pole region\n",
    "HiggsM_tail_low = np.linspace(HiggsM_min, lower_bound, num_samples_per_region)\n",
    "HiggsM_pole = np.linspace(lower_bound, upper_bound, num_samples_per_region)\n",
    "HiggsM_tail_high = np.linspace(upper_bound, HiggsM_max, num_samples_per_region)\n",
    "HiggsM_sample = np.concatenate([HiggsM_tail_low, HiggsM_pole, HiggsM_tail_high])\n",
    "\n",
    "# Normalize HiggsM\n",
    "HiggsM_sample_norm = (HiggsM_sample - HiggsM_min) / (HiggsM_max - HiggsM_min)\n",
    "\n",
    "# Initialize noise\n",
    "xt = torch.randn(len(HiggsM_sample), 1)\n",
    "cond = torch.Tensor(HiggsM_sample_norm[:, None])\n",
    "\n",
    "# Sampling process\n",
    "plot_every = 100\n",
    "for step, t in enumerate(torch.linspace(0, 1, steps), start=1):\n",
    "    with torch.no_grad():\n",
    "        pred = model(xt, t.expand(xt.size(0)), cond)\n",
    "    xt = xt + (1 / steps) * pred\n",
    "    if step % plot_every == 0:\n",
    "        # Denormalize frequency predictions back to original scale\n",
    "        xt_denorm = xt.detach().numpy() * (Freq_max - Freq_min) + Freq_min\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.scatter(sampled_points[:, 0], sampled_points[:, 1], color=\"red\", marker=\"o\", label=\"Training data (tails)\", alpha=0.6)\n",
    "        plt.scatter(HiggsM_sample, xt_denorm, color=\"green\", marker=\".\", label=\"Generated samples\", alpha=0.3)\n",
    "        plt.axvline(lower_bound, color='blue', linestyle='--', alpha=0.5, label='Pole region')\n",
    "        plt.axvline(upper_bound, color='blue', linestyle='--', alpha=0.5)\n",
    "        plt.xlabel('HiggsM')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.title(f'Conditional Flow Matching - Step {step}/{steps}')\n",
    "        plt.show()\n",
    "\n",
    "model.train().requires_grad_(True)\n",
    "print(\"Done Sampling - Model learned to extrapolate from tails to pole region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd6ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHP_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
