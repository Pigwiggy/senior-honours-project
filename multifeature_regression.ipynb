{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d01256",
   "metadata": {},
   "source": [
    "# Regression Multifeature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba41be9",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Data preparation: Now keeps only tail data and normalizes HiggsM values for conditioning.\n",
    "\n",
    "Model: ConditionalMLP now accepts multiple inputs based on the number of features:\n",
    "\n",
    "The HiggsM conditioning information is projected and added to the network, allowing the model to learn how the velocity field varies with HiggsM.\n",
    "\n",
    "Training: Now passes the corresponding HiggsM value for each sample during training.\n",
    "\n",
    "Sampling (cell 12): Samples across the entire HiggsM range including the pole region. The model uses learned patterns from tail data to extrapolate predictions into the pole region where it has no direct training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae51fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "raw_df = pd.read_hdf('data_files/new_Input_NonResonant_yy_25th_January2026.h5', key='VBF_Polarisation_Tree')\n",
    "\n",
    "# Define bounds and number of bins\n",
    "lower_bound = 115000\n",
    "upper_bound = 135000\n",
    "num_bins = 1100\n",
    "\n",
    "# Creating frequency distribution for HiggsM\n",
    "freq = pd.cut(raw_df['HiggsM'], bins=num_bins)\n",
    "freq_counts = freq.value_counts().sort_index()\n",
    "higgsM_values = [interval.mid for interval in freq_counts.index]\n",
    "frequencies = freq_counts.values.tolist()\n",
    "\n",
    "# Create DataFrame for ML Model\n",
    "# Applying the bounds to HiggsM values\n",
    "model_df = pd.DataFrame({'HiggsM': higgsM_values, 'Frequency': frequencies})\n",
    "mask1 = model_df['HiggsM'] > upper_bound\n",
    "mask2 = model_df['HiggsM'] < lower_bound\n",
    "model_df_tails = model_df[mask1 | mask2]\n",
    "\n",
    "# Applying the same bounds to the raw data for the other features\n",
    "grouped_df = raw_df.groupby(pd.cut(raw_df['HiggsM'], bins=num_bins), observed=True).mean(numeric_only=True)\n",
    "grouped_df.index.name = 'HiggsM_interval'\n",
    "grouped_df = grouped_df.reset_index()\n",
    "grouped_df['HiggsM_interval'] = grouped_df['HiggsM_interval'].apply(lambda x: x.mid).astype(float)\n",
    "mask3 = grouped_df['HiggsM_interval'] > upper_bound\n",
    "mask4 = grouped_df['HiggsM_interval'] < lower_bound\n",
    "others_df_tails = grouped_df[mask3 | mask4]\n",
    "\n",
    "# Combine tail data for other features and HiggsM frequency into a single DataFrame for training\n",
    "model_df_tails.index.name = others_df_tails.index.name\n",
    "combined_tails_df = pd.concat([model_df_tails, others_df_tails.drop(columns=['HiggsM', 'HiggsM_interval'])], axis=1)\n",
    "\n",
    "# Save the combined tails DataFrame to a CSV file for inspection\n",
    "# Filter out unnecessary columns and save the relevant features along with HiggsM and Frequency\n",
    "combined_tails_df.to_csv('data_dump/combined_tails_data.csv')\n",
    "features = ['DNN_score', 'Frequency', 'DPhi_jj', 'Eta_jj', 'M_jj', 'Njets', 'OO1']\n",
    "features_array = combined_tails_df[features].to_numpy()\n",
    "num_features = features_array.shape[1]\n",
    "# print(features_array.shape)\n",
    "# print(features_array[:5, :])\n",
    "\n",
    "# Array of HiggsM values for training\n",
    "HiggsM_array = combined_tails_df['HiggsM'].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee819002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Normalise function\n",
    "# Un-nomralise later using sampled_points_original = scaler.inverse_transform(sampled_points_normalized)\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_HiggsM = MinMaxScaler()\n",
    "\n",
    "# Normalise features and HiggsM separately\n",
    "features_normalised = scaler_features.fit_transform(features_array)\n",
    "HiggsM_normalised = scaler_HiggsM.fit_transform(HiggsM_array.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad6471",
   "metadata": {},
   "source": [
    "### Random noise data points to start with, this is the initial distribution p0(x0), original code used random noise, replace with the Higgs tail data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6e028",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### The multi-feature model has input channels based on the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7990f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a4329",
   "metadata": {},
   "source": [
    "### Taking output data and comparing it to true data by saving both as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cf964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalising the values produced by the model\n",
    "HiggsM_denorm = scaler_HiggsM.inverse_transform(HiggsM_sample_norm.reshape(-1, 1)).flatten()\n",
    "features_denorm = scaler_features.inverse_transform(xt.detach().cpu().numpy())\n",
    "\n",
    "pred_points = pd.DataFrame({'HiggsM': HiggsM_denorm,\n",
    "                            'DNN_score': features_denorm[:, 0],\n",
    "                            'Frequency': features_denorm[:, 1],\n",
    "                            'DPhi_jj': features_denorm[:, 2],\n",
    "                            'Eta_jj': features_denorm[:, 3], \n",
    "                            'M_jj': features_denorm[:, 4],\n",
    "                            'Njets': features_denorm[:, 5],\n",
    "                            'OO1': features_denorm[:, 6],\n",
    "                            })\n",
    "\n",
    "# Constructing the pole region data for both the predicted and true data\n",
    "model_df_pole = model_df[~(mask1 | mask2)]\n",
    "others_df_pole = grouped_df[~(mask3 | mask4)]\n",
    "model_df_pole.index.name = others_df_pole.index.name\n",
    "true_points = pd.concat([model_df_pole, others_df_pole.drop(columns=['HiggsM', 'HiggsM_interval'])], axis=1)\n",
    "variables = ['HiggsM', 'DNN_score', 'Frequency', 'DPhi_jj', 'Eta_jj', 'M_jj', 'Njets', 'OO1']\n",
    "true_points = true_points[variables]\n",
    "lower_bound = 115000\n",
    "upper_bound = 135000\n",
    "pred_points_pole = pred_points[(pred_points['HiggsM'] >= lower_bound) & (pred_points['HiggsM'] <= upper_bound)]\n",
    "\n",
    "# Saving data to csv for comparison\n",
    "csv1 = pd.DataFrame(pred_points_pole, columns=['HiggsM', 'DNN_score', 'Frequency', 'DPhi_jj', 'Eta_jj', 'M_jj', 'Njets', 'OO1'])\n",
    "csv2 = pd.DataFrame(true_points, columns=['HiggsM', 'DNN_score', 'Frequency', 'DPhi_jj', 'Eta_jj', 'M_jj', 'Njets', 'OO1'])\n",
    "csv1.to_csv('data_dump/multi_predicted_pole_region.csv', index=False)\n",
    "csv2.to_csv('data_dump/multi_true_pole_region.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07491b0c",
   "metadata": {},
   "source": [
    "## KL Divergence calculation to assess quality of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 8) (400, 8)\n",
      "KL Divergence between true and predicted distributions in the pole region: 9.022945566141522e-09 for feature DNN_score\n",
      "KL Divergence between true and predicted distributions in the pole region: 0.00011006198634116359 for feature Frequency\n",
      "KL Divergence between true and predicted distributions in the pole region: 0.001026079114787892 for feature DPhi_jj\n",
      "KL Divergence between true and predicted distributions in the pole region: 1.6471510407342e-05 for feature Eta_jj\n",
      "KL Divergence between true and predicted distributions in the pole region: 1.2014802021659376e-05 for feature M_jj\n",
      "KL Divergence between true and predicted distributions in the pole region: 8.364150537747126e-05 for feature Njets\n",
      "KL Divergence between true and predicted distributions in the pole region: 1.9099076413029158e-05 for feature OO1\n"
     ]
    }
   ],
   "source": [
    "# Ensuring that both arrays have the same shape for KL Divergence calculation by trimming both ends\n",
    "counter = 0\n",
    "if pred_points_pole.shape > true_points.shape:\n",
    "    while pred_points_pole.shape != true_points.shape:\n",
    "        if counter % 2 == 0: \n",
    "            pred_points_pole = np.delete(pred_points_pole, -1, axis=0)\n",
    "            counter += 1\n",
    "        else:\n",
    "            pred_points_pole = np.delete(pred_points_pole, 0, axis=0)\n",
    "        counter += 1\n",
    "\n",
    "elif pred_points_pole.shape < true_points.shape:\n",
    "    while pred_points_pole.shape != true_points.shape:\n",
    "        if counter % 2 == 0: \n",
    "            true_points = np.delete(true_points, -1, axis=0)\n",
    "            counter += 1\n",
    "        else:\n",
    "            true_points = np.delete(true_points, 0, axis=0)\n",
    "            counter += 1\n",
    "        \n",
    "\n",
    "# Convert true_points to a numpy array, pred_points_pole is already a numpy array\n",
    "true_points_array = true_points.to_numpy()\n",
    "print(true_points_array.shape, pred_points_pole.shape)\n",
    "\n",
    "# Implement KL Divergence calculation\n",
    "for i in range(len(features)):\n",
    "    true_sum = np.sum(true_points_array[:, i])\n",
    "    p = np.array(true_points_array[:, i]) / true_sum\n",
    "    pred_sum = np.sum(pred_points_pole[:, i])\n",
    "    q = np.array(pred_points_pole[:, i]) / pred_sum\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    p_smooth = p + epsilon\n",
    "    q_smooth = q + epsilon\n",
    "\n",
    "    kl_divergence = np.sum(p_smooth * np.log(p_smooth / q_smooth))\n",
    "    \n",
    "    \n",
    "    if np.isinf(kl_divergence) or np.isnan(kl_divergence):\n",
    "        error_df = pd.DataFrame(p_smooth/q_smooth)\n",
    "        error_df.to_csv('data_dump/check.csv', index=False)\n",
    "\n",
    "\n",
    "    if i==3:\n",
    "        error_df = pd.DataFrame(p_smooth/q_smooth)\n",
    "        error_df.to_csv('data_dump/check1.csv', index=False)\n",
    "\n",
    "\n",
    "    print(f\"KL Divergence between true and predicted distributions in the pole region: {kl_divergence} for feature {features[i]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHP_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
